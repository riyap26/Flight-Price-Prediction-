{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/riyapatel/Downloads/backwards.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riyapatel/Downloads/backwards.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m features_to_use \u001b[39m=\u001b[39m best_features \u001b[39m+\u001b[39m [feature]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riyapatel/Downloads/backwards.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Fit the model with the selected features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/riyapatel/Downloads/backwards.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train[features_to_use], y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riyapatel/Downloads/backwards.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riyapatel/Downloads/backwards.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mscore(X_test[features_to_use], y_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:390\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSum of y is not strictly positive which \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis necessary for Poisson regression.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 390\u001b[0m y, expanded_class_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_y_class_weight(y)\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(y, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m!=\u001b[39m DOUBLE \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m y\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mcontiguous:\n\u001b[1;32m    393\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(y, dtype\u001b[39m=\u001b[39mDOUBLE)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:749\u001b[0m, in \u001b[0;36mForestClassifier._validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_y_class_weight\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m--> 749\u001b[0m     check_classification_targets(y)\n\u001b[1;32m    751\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(y)\n\u001b[1;32m    752\u001b[0m     expanded_class_weight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/multiclass.py:215\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    207\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[1;32m    209\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    214\u001b[0m ]:\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m{\u001b[39;00my_type\u001b[39m}\u001b[39;00m\u001b[39m. Maybe you are trying to fit a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mclassifier, which expects discrete classes on a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mregression target with continuous values.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load your dataset (replace 'data.csv' with your dataset file)\n",
    "data = pd.read_csv('newestest_itineraries4.csv')\n",
    "\n",
    "# Extract the feature labels from the first row\n",
    "feature_labels = data.columns[1:].values  # Exclude the first column (target) for feature labels\n",
    "\n",
    "# Set the first column as the target variable\n",
    "X = data.iloc[:, 1:]  # Exclude the first column (target) for features\n",
    "y = data.iloc[:, 0]  # Use the first column as the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an initial model (you can choose a different model based on your dataset)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Store the original feature list\n",
    "remaining_features = list(X.columns)\n",
    "\n",
    "# Define variables to store feature rankings and their corresponding scores\n",
    "feature_rankings = {}\n",
    "best_features = []\n",
    "\n",
    "# Set the number of features you want to select (in this case, 10)\n",
    "num_features_to_select = 10\n",
    "\n",
    "while len(remaining_features) > num_features_to_select:\n",
    "    scores = []\n",
    "\n",
    "    # Train and evaluate the model with each remaining feature removed one at a time\n",
    "    for feature in remaining_features:\n",
    "        features_to_use = best_features + [feature]\n",
    "\n",
    "        # Fit the model with the selected features\n",
    "        model.fit(X_train[features_to_use], y_train)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        score = model.score(X_test[features_to_use], y_test)\n",
    "        scores.append((feature, score))\n",
    "\n",
    "    # Find the feature with the lowest score (least important)\n",
    "    worst_feature, worst_score = min(scores, key=lambda x: x[1])\n",
    "\n",
    "    # Remove the least important feature from the remaining features\n",
    "    remaining_features.remove(worst_feature)\n",
    "\n",
    "    # Store the feature ranking\n",
    "    feature_rankings[worst_feature] = worst_score\n",
    "\n",
    "    # Store the best features so far\n",
    "    best_features = [f for f in best_features if f != worst_feature]\n",
    "\n",
    "# Select the top N features based on their scores\n",
    "selected_features = list(sorted(feature_rankings, key=feature_rankings.get, reverse=True)[:num_features_to_select])\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sort the feature rankings by importance score\n",
    "sorted_feature_rankings = {k: v for k, v in sorted(feature_rankings.items(), key=lambda item: item[1])}\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_feature_rankings)), list(sorted_feature_rankings.values()), align='center')\n",
    "plt.yticks(range(len(sorted_feature_rankings)), list(sorted_feature_rankings.keys()))\n",
    "plt.xlabel('Model Score (Higher is better)')\n",
    "plt.title('Feature Importance Scores in Backward Feature Selection')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
